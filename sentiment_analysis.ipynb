{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Analysis Report\n",
        "\n",
        "This notebook analyzes survey responses using a state-of-the-art RoBERTa-based sentiment analysis model.\n",
        "\n",
        "## Instructions\n",
        "1. Place your survey data in a CSV file with a column named 'text' or 'response'\n",
        "2. Update the `DATA_FILE` variable below with your file path\n",
        "3. Run all cells to generate the sentiment analysis report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_FILE = 'survey_data.csv'  # Update this with your survey data file path\n",
        "MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'  # State-of-the-art RoBERTa model\n",
        "\n",
        "# Check if CUDA is available for GPU acceleration\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU (CUDA)' if device == 0 else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the sentiment analysis pipeline\n",
        "print(\"Loading sentiment analysis model...\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=MODEL_NAME,\n",
        "    tokenizer=MODEL_NAME,\n",
        "    device=device,\n",
        "    return_all_scores=True\n",
        ")\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load survey data\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE)\n",
        "    print(f\"Data loaded successfully! Found {len(df)} responses.\")\n",
        "    print(f\"\\nColumns in dataset: {list(df.columns)}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{DATA_FILE}' not found.\")\n",
        "    print(\"Please update the DATA_FILE variable with the correct path to your survey data.\")\n",
        "    print(\"\\nExpected CSV format:\")\n",
        "    print(\"- Column named 'text' or 'response' containing the survey responses\")\n",
        "    print(\"- Additional columns (optional): date, respondent_id, question_id, etc.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify the text column\n",
        "text_column = None\n",
        "for col in ['text', 'response', 'comment', 'feedback', 'answer']:\n",
        "    if col in df.columns:\n",
        "        text_column = col\n",
        "        break\n",
        "\n",
        "if text_column is None:\n",
        "    print(\"Available columns:\", list(df.columns))\n",
        "    text_column = input(\"Enter the name of the column containing text responses: \")\n",
        "\n",
        "print(f\"Using column '{text_column}' for sentiment analysis\")\n",
        "\n",
        "# Clean the data - remove empty responses\n",
        "df_clean = df[df[text_column].notna() & (df[text_column].str.strip() != '')].copy()\n",
        "print(f\"\\nCleaned data: {len(df_clean)} valid responses (removed {len(df) - len(df_clean)} empty responses)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform sentiment analysis\n",
        "print(\"Analyzing sentiment for all responses...\")\n",
        "print(\"This may take a few minutes depending on the number of responses...\")\n",
        "\n",
        "sentiments = []\n",
        "scores = []\n",
        "\n",
        "for text in tqdm(df_clean[text_column], desc=\"Processing responses\"):\n",
        "    try:\n",
        "        # Get sentiment scores\n",
        "        result = sentiment_pipeline(text[:512])  # Limit to 512 characters for model input\n",
        "        \n",
        "        # Extract the highest confidence sentiment\n",
        "        best_sentiment = max(result[0], key=lambda x: x['score'])\n",
        "        sentiments.append(best_sentiment['label'])\n",
        "        scores.append(best_sentiment['score'])\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing text: {str(e)}\")\n",
        "        sentiments.append('ERROR')\n",
        "        scores.append(0.0)\n",
        "\n",
        "# Add results to dataframe\n",
        "df_clean['sentiment'] = sentiments\n",
        "df_clean['sentiment_score'] = scores\n",
        "\n",
        "print(\"\\nSentiment analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map sentiment labels to standard format (POSITIVE, NEGATIVE, NEUTRAL)\n",
        "# The RoBERTa model uses LABEL_0, LABEL_1, LABEL_2 format\n",
        "# We need to map them to readable labels\n",
        "\n",
        "# Get a sample to understand the label format\n",
        "sample_result = sentiment_pipeline(\"I love this product!\")\n",
        "print(\"Sample sentiment labels:\")\n",
        "for item in sample_result[0]:\n",
        "    print(f\"  {item['label']}: {item['score']:.4f}\")\n",
        "\n",
        "# Map labels based on the model's output\n",
        "# The twitter-roberta-base-sentiment-latest model uses: LABEL_0 (negative), LABEL_1 (neutral), LABEL_2 (positive)\n",
        "label_mapping = {\n",
        "    'LABEL_0': 'NEGATIVE',\n",
        "    'LABEL_1': 'NEUTRAL',\n",
        "    'LABEL_2': 'POSITIVE'\n",
        "}\n",
        "\n",
        "# Apply mapping\n",
        "df_clean['sentiment_label'] = df_clean['sentiment'].map(label_mapping).fillna(df_clean['sentiment'])\n",
        "\n",
        "print(\"\\nSentiment distribution:\")\n",
        "print(df_clean['sentiment_label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary Statistics\n",
        "print(\"=\" * 60)\n",
        "print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total Responses Analyzed: {len(df_clean)}\")\n",
        "print(f\"\\nSentiment Distribution:\")\n",
        "sentiment_counts = df_clean['sentiment_label'].value_counts()\n",
        "for sentiment, count in sentiment_counts.items():\n",
        "    percentage = (count / len(df_clean)) * 100\n",
        "    print(f\"  {sentiment}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "print(f\"\\nAverage Sentiment Score: {df_clean['sentiment_score'].mean():.4f}\")\n",
        "print(f\"Median Sentiment Score: {df_clean['sentiment_score'].median():.4f}\")\n",
        "\n",
        "# Calculate sentiment ratio\n",
        "positive_count = len(df_clean[df_clean['sentiment_label'] == 'POSITIVE'])\n",
        "negative_count = len(df_clean[df_clean['sentiment_label'] == 'NEGATIVE'])\n",
        "neutral_count = len(df_clean[df_clean['sentiment_label'] == 'NEUTRAL'])\n",
        "\n",
        "if negative_count > 0:\n",
        "    pos_neg_ratio = positive_count / negative_count\n",
        "    print(f\"\\nPositive to Negative Ratio: {pos_neg_ratio:.2f}\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 1: Sentiment Distribution Pie Chart\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Pie chart\n",
        "sentiment_counts = df_clean['sentiment_label'].value_counts()\n",
        "colors = {'POSITIVE': '#2ecc71', 'NEGATIVE': '#e74c3c', 'NEUTRAL': '#95a5a6'}\n",
        "pie_colors = [colors.get(sent, '#3498db') for sent in sentiment_counts.index]\n",
        "\n",
        "axes[0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', \n",
        "            colors=pie_colors, startangle=90)\n",
        "axes[0].set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Bar chart\n",
        "axes[1].bar(sentiment_counts.index, sentiment_counts.values, \n",
        "            color=[colors.get(sent, '#3498db') for sent in sentiment_counts.index])\n",
        "axes[1].set_title('Sentiment Counts', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Number of Responses')\n",
        "axes[1].set_xlabel('Sentiment')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 2: Sentiment Score Distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(df_clean['sentiment_score'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Distribution of Sentiment Confidence Scores', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Confidence Score')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].axvline(df_clean['sentiment_score'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df_clean[\"sentiment_score\"].mean():.3f}')\n",
        "axes[0].legend()\n",
        "\n",
        "# Box plot by sentiment\n",
        "sentiment_order = ['POSITIVE', 'NEUTRAL', 'NEGATIVE']\n",
        "sentiment_order = [s for s in sentiment_order if s in df_clean['sentiment_label'].values]\n",
        "df_clean_sorted = df_clean[df_clean['sentiment_label'].isin(sentiment_order)]\n",
        "\n",
        "box_data = [df_clean_sorted[df_clean_sorted['sentiment_label'] == sent]['sentiment_score'].values \n",
        "            for sent in sentiment_order]\n",
        "axes[1].boxplot(box_data, labels=sentiment_order)\n",
        "axes[1].set_title('Sentiment Score Distribution by Category', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Confidence Score')\n",
        "axes[1].set_xlabel('Sentiment')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample responses by sentiment\n",
        "print(\"=\" * 60)\n",
        "print(\"SAMPLE RESPONSES BY SENTIMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for sentiment in ['POSITIVE', 'NEUTRAL', 'NEGATIVE']:\n",
        "    if sentiment in df_clean['sentiment_label'].values:\n",
        "        print(f\"\\n{sentiment} Responses (Top 3 by confidence):\")\n",
        "        sentiment_df = df_clean[df_clean['sentiment_label'] == sentiment].nlargest(3, 'sentiment_score')\n",
        "        for idx, row in sentiment_df.iterrows():\n",
        "            text_preview = str(row[text_column])[:100] + \"...\" if len(str(row[text_column])) > 100 else str(row[text_column])\n",
        "            print(f\"  [{row['sentiment_score']:.3f}] {text_preview}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results to CSV\n",
        "output_file = f'sentiment_analysis_results_{pd.Timestamp.now().strftime(\"%Y%m%d\")}.csv'\n",
        "df_clean.to_csv(output_file, index=False)\n",
        "print(f\"Results exported to: {output_file}\")\n",
        "print(f\"\\nFile contains {len(df_clean)} rows with sentiment analysis results.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a summary report text file\n",
        "report_file = f'sentiment_report_{pd.Timestamp.now().strftime(\"%Y%m%d\")}.txt'\n",
        "\n",
        "with open(report_file, 'w') as f:\n",
        "    f.write(\"SENTIMENT ANALYSIS REPORT\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "    f.write(f\"Report Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "    f.write(f\"Total Responses Analyzed: {len(df_clean)}\\n\\n\")\n",
        "    f.write(\"Sentiment Distribution:\\n\")\n",
        "    sentiment_counts = df_clean['sentiment_label'].value_counts()\n",
        "    for sentiment, count in sentiment_counts.items():\n",
        "        percentage = (count / len(df_clean)) * 100\n",
        "        f.write(f\"  {sentiment}: {count} ({percentage:.2f}%)\\n\")\n",
        "    f.write(f\"\\nAverage Sentiment Score: {df_clean['sentiment_score'].mean():.4f}\\n\")\n",
        "    f.write(f\"Median Sentiment Score: {df_clean['sentiment_score'].median():.4f}\\n\")\n",
        "    \n",
        "    if negative_count > 0:\n",
        "        pos_neg_ratio = positive_count / negative_count\n",
        "        f.write(f\"\\nPositive to Negative Ratio: {pos_neg_ratio:.2f}\\n\")\n",
        "\n",
        "print(f\"Summary report saved to: {report_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
